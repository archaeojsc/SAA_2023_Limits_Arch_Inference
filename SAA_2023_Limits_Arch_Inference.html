<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Why is there Math in my Archaeology?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="SAA_2023_Limits_Arch_Inference_files/libs/clipboard/clipboard.min.js"></script>
<script src="SAA_2023_Limits_Arch_Inference_files/libs/quarto-html/quarto.js"></script>
<script src="SAA_2023_Limits_Arch_Inference_files/libs/quarto-html/popper.min.js"></script>
<script src="SAA_2023_Limits_Arch_Inference_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="SAA_2023_Limits_Arch_Inference_files/libs/quarto-html/anchor.min.js"></script>
<link href="SAA_2023_Limits_Arch_Inference_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="SAA_2023_Limits_Arch_Inference_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="SAA_2023_Limits_Arch_Inference_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="SAA_2023_Limits_Arch_Inference_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="SAA_2023_Limits_Arch_Inference_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Why is there Math in my Archaeology?</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Fifty years ago, what arguably could have been one of the most important papers written for modern work in quantitative archaeology was published in American Antiquity. Unfortunately for its author, and generations of archaeologists, it received relatively little attention at the time. With few citations, more than half of which have occurred in just the last few years, its elegance and mathematical precision went largely unappreciated.</p>
<p>John Justeson’s article “Limitations of archaeological inference: an information-theoretic approach with applications in methodology” <span class="citation" data-cites="Justeson1973">(<a href="#ref-Justeson1973" role="doc-biblioref">1973</a>)</span> was rather ambitious, as can be seen from its abstract:</p>
<blockquote class="blockquote">
<p>“A framework is established for the application of information-theoretic concepts to the study of archaeological inference, ultimately to provide an estimate of the degree to which archaeologists, or anthropologists in general, can provide legitimate answers to the questions they investigate. Particular information-theoretic measures are applied to the design elements on the ceramics of a southwestern pueblo to show the methodological utility of information theory in helping to reach closer to that limit.” <span class="citation" data-cites="Justeson1973">(<a href="#ref-Justeson1973" role="doc-biblioref">Justeson 1973</a>)</span></p>
</blockquote>
<p>The premise was actually quite straightforward – behavioral information is “encoded” in the material artifacts deposited within an archaeological site, and the archaeologist’s goal is to “decode” that information on the other end. The novelty was that John saw this “encoding-decoding” process as an information flow that could be described by what was (at the time) a relatively esoteric set of mathematical tools known as <em>information theory</em>.</p>
<p>The foundations of information theory were developed by Claude Shannon as a way to analyze the transmission of information <em>independently</em> of the content of a message.</p>
<blockquote class="blockquote">
<p>“The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities.” <span class="citation" data-cites="Shannon1948">(<a href="#ref-Shannon1948" role="doc-biblioref">Shannon 1948, 1</a>)</span></p>
</blockquote>
<p>Justeson saw that this approach might also be used to establish an “upper limit” for how interpretable archaeological data could be. Moreover, he demonstrated that we could reasonably calculate a quantifiable <em>measurement</em> for that upper limit form those data.</p>
<blockquote class="blockquote">
<p>“If the empirically measured parameters are not consistent with the relationship between them that is required by the theory for a given material or behavioral system, then the data by which that system is to be interpreted cannot have a consistent susceptibility to decoding; that is, there will be no basis for deriving a coherent archaeological interpretation of the data that will accurately reflect the prehistoric situation.” <span class="citation" data-cites="Justeson1973">(<a href="#ref-Justeson1973" role="doc-biblioref">Justeson 1973, 136</a>)</span></p>
</blockquote>
<p>In other words, observed archaeological features or attributes should represent a coherent and systematic pattern of activities.</p>
<p>In particular, he was addressing two <em>inherent</em> limitations of the archaeological record:</p>
<ol type="1">
<li>limitations imposed by the degree of preservation of culturally significant remains and by the skewing of their relationships through time until their recovery; and</li>
<li>limitations on the interpretability of archaeological data for the cultural descriptions.</li>
</ol>
<p>The first limitation is analogous degradation of a signal due to noise or interference affecting a transmission, and the second to the encoding and decoding of that signal between sender and receiver.</p>
<p><span class="citation" data-cites="Schiffer1972">Schiffer (<a href="#ref-Schiffer1972" role="doc-biblioref">1972</a>)</span> had previously elaborated on the distinction between <em>systemic</em> and <em>archaeological</em> contexts, differentiating between the cultural and taphonomic processes that create the observable archaeological record. It would not be until a decade later <span class="citation" data-cites="Schiffer1983 Schiffer1987">(<a href="#ref-Schiffer1983" role="doc-biblioref">Schiffer 1983</a>, <a href="#ref-Schiffer1987" role="doc-biblioref">1987</a>)</span> that he would formalize these as <em>natural</em> versus <em>cultural</em> transformation processes (i.e., <span class="math inline">\(n\)</span>-transforms and <span class="math inline">\(c\)</span>-transforms). <span class="citation" data-cites="Justeson1973">Justeson (<a href="#ref-Justeson1973" role="doc-biblioref">1973</a>)</span> …</p>
</section>
<section id="a-gentle-introduction-to-information-theory" class="level2">
<h2 class="anchored" data-anchor-id="a-gentle-introduction-to-information-theory">A Gentle Introduction to Information Theory</h2>
<p>What is now known as <em>Information Theory</em> largely began with a seminal paper written by Claude Shannon, titled “A Mathematical Theory of Communication” <span class="citation" data-cites="Shannon1948">(<a href="#ref-Shannon1948" role="doc-biblioref">1948</a>)</span> resulting from his work in cryptography at Bell Labs. At the heart of Shannon’s theory was the idea that <em>information</em> is fundamentally tied to the reduction of <em>uncertainty</em>. Shannon approached information not in terms of meaning, but as a measure of the <em>reduction of uncertainty</em> within a system of communication.</p>
<p>Specifically, he proposed a particular relationship between information and uncertainty in terms of statistical probabilities. He derived a quantitative measure of that uncertainty derived from the concept of <em>entropy</em> used to describe disorder in the thermodynamics of physical systems. Shannon, however, repurposed entropy to refer to the average uncertainty contained in a system given by the equation:</p>
<p><span class="math display">\[
H(\cal{X}) = - \sum_{i=1}^{n} p(x_i) \ \log_2 \ p(x_i)
\]</span></p>
<p>What this equation is describing is the total entropy <span class="math inline">\(H\)</span> of some system <span class="math inline">\(\cal{X}\)</span> that contains <span class="math inline">\(n\)</span> discrete attributes or elements <span class="math inline">\((x_{1}, x_{2}, \ldots x_{n})\)</span>. This is defined as the negative sum over all features of each element’s probability of occurrence <span class="math inline">\(p(x_i)\)</span> times the <span class="math inline">\(\log_2\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> of that probability.</p>
<p>The higher the entropy of a system, indicated by a higher value of <span class="math inline">\(H\)</span>, the more uncertainty or randomness there is to the elements of <span class="math inline">\(\cal{X}\)</span>. Somewhat counterintuitively, the more uncertain or random a system the more information it conveys. Remember that Shannon defines information as the reduction of uncertainty. The greater the uncertainty (i.e., high entropy), the more potential information the system is capable of producing because there is more uncertainty to reduce.</p>
<p>To see how, we need to understand what Shannon defined as <em>surprisal</em>. Surprisal, also known as self-information, is a measure of how surprising or unexpected a specific event is based on its probability. In essence, surprisal measures the information content of a specific outcome – i.e., rare events carry more information than common ones because they are less expected. Low probability events, those that occur infrequently, are highly surprising. Conversely, high probability events are not.</p>
<p>Consider it this way – if an event is nearly certain to occur, you would <em>already</em> be expecting it to happen when it does. Its occurrence tells you nothing that you did not already know. It is only when something happens that we did <em>not</em> expect (i.e., we are surprised) that it is providing <em>new</em> information. Therefore, surprisal (denoted as <span class="math inline">\(I(x)\)</span>) is the potential <em>information</em> contained in a single event based on its probability <span class="math inline">\(p(x)\)</span>:</p>
<p><span class="math display">\[ I(x) = - \log_2 p(x) \]</span></p>
<p>Surprisal is zero for events that are certain (i.e., the probability <span class="math inline">\(p(x)=1\)</span>), and grows larger as the probability of the event decreases (Figure {#figure:surprisal_example}). Exceedingly rare events, by contrast, would be very surprising to witness and approaching “infinitely” surprising as the probability of the event goes to zero (i.e., <span class="math inline">\(lim_{p(x) \to 0} \ I(x) = \infty\)</span>).</p>
<div id="figure:surprisal_example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./surprisal.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The surprisal <span class="math inline">\(I(x)\)</span> of a coin flip <span class="math inline">\(x\)</span> (i.e., lands “heads” or “tails”) as the probability (<span class="math inline">\(p(x)\)</span>) of landing “heads” ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> for a “biased” coin. A “fair” coin would land on heads or tails with equal chances (<span class="math inline">\(p(x)=0.5\)</span>).</figcaption>
</figure>
</div>
<p>Entropy represents the <em>average</em> surprisal over all possible outcomes from a probability distribution. It quantifies the overall uncertainty or unpredictability of a system or source of information. The higher the entropy, the more information the system is capable of producing, since there is greater uncertainty about which outcome will occur.</p>
<p>Entropy is highest when all outcomes are equally likely, and decreases as we gain more information to anticipate whether or not that event is likely to occur (Figure {#figure:entropy_example}). Information is therefore the reduction of that uncertainty or entropy when a new event is observed. We have learned more about the underlying probabilities for future events.</p>
<div id="figure:entropy_example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./Entropy.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The overall system entropy <span class="math inline">\(H(\cal{X})\)</span> for biased coin flips as the probability (<span class="math inline">\(p(x)\)</span>) of landing “heads” ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>. A “fair” coin (<span class="math inline">\(p(x)=0.5\)</span>) is the system with the most uncertainty, since either outcome (“heads” or “tails”) is equally possible.</figcaption>
</figure>
</div>
<p>For the first time, scientists had a way to <em>quantify</em> information. Shannon had defined information in a way that made it possible to measure and <em>analyze</em> it mathematically, based solely on its statistical structure and independently of its content or meaning.</p>
</section>
<section id="information-theory-in-archaeology" class="level2">
<h2 class="anchored" data-anchor-id="information-theory-in-archaeology">Information Theory in Archaeology</h2>
<!-- ### The Evolution and Critical Debate of Information Theory in Archaeology -->
<!-- The integration of information theory into archaeological research has evolved significantly since its early application in the 1970s. Inspired by Shannon's foundational ideas on information, entropy, and communication channels, archaeologists have employed these concepts to analyze the transmission of cultural traits, the integrity of artifacts, and the uncertainty inherent in archaeological data. However, the application of these ideas has sparked substantial debate regarding their limitations in addressing the complexities of human behavior and cultural evolution.

Michael Schiffer's pioneering work [-@Schiffer1972] marked one of the earliest uses of information theory in archaeology, where he explored the disruption of information flow caused by post-depositional processes, termed as "systemic and archaeological contexts." Schiffer emphasized how the information contained within artifacts could degrade over time due to various environmental and cultural factors, introducing "noise" into the archaeological record. This idea aligned with Shannon's theory of communication, where noise distorts messages as they pass through a channel. Schiffer's subsequent work [-@Schiffer1983] on formation processes expanded on this, demonstrating how entropy, a measure of disorder, influences the amount of reliable information that survives in archaeological contexts.

Michael Schiffer's seminal work "Formation Processes of the Archaeological Record" [-@Schiffer1987] is among the most influential applications of information theory concepts to archaeology, even though Schiffer did not explicitly use Shannon's framework. Schiffer introduced the idea that archaeological sites are the result of two key processes: cultural formation processes, which involve human behaviors that create and modify archaeological deposits, and natural formation processes, which are the non-human agents, such as erosion or animal activity, that affect the archaeological record. The notion of information loss in these processes echoes Shannon's concepts of entropy and noise. Schiffer's emphasis on understanding how archaeological data are transformed before and after deposition mirrors the concerns of information theory regarding how signals are distorted through transmission. By conceptualizing the archaeological record as a series of transformations from its original state, Schiffer advanced a model that paralleled information processing, where each formation process acts as a filter, introducing "noise" and altering the original "message."

John Justeson [-@Justeson1973] applied Shannon's concepts more directly to archaeological inference, investigating the limitations of reconstructing past human behavior through fragmentary data. He focused on how entropy could quantify uncertainty and signal degradation, critiquing the oversimplification that sometimes results when human complexity is reduced to mathematical models. This early work identified the tension between abstract, quantitative frameworks and the nuances of cultural history, a critique that has persisted in the field.

John Justeson's The Limitations of Archaeological Interpretation [-@Justeson1973] explicitly drew on Shannon's ideas, applying them to the challenges archaeologists face in reconstructing past human behaviors from material remains. Justeson argued that archaeological interpretation is fundamentally limited by the quantity of information that can be extracted from the archaeological record. He viewed the archaeological record as a degraded and incomplete set of signals, with each artifact or feature representing a small, noisy fraction of the original cultural system. Justeson applied Shannon's concept of entropy to assess the degree of uncertainty in archaeological interpretations, highlighting how increasing entropy in the archaeological record—due to processes such as taphonomy or excavation biases—leads to more ambiguous or unreliable interpretations.

Justeson's work helped establish a dialogue within archaeology about the inherent limitations of inference from incomplete datasets, contributing to the development of more cautious and methodologically rigorous approaches to interpreting the archaeological record. His use of Shannon's ideas encouraged archaeologists to critically evaluate the reliability of their data and the extent to which they could justifiably infer past behaviors or cultural practices.

In the 1980s, other scholars applied information theory to model cultural interaction. @Dickens1984 used Shannon's idea of channel capacity to study the flow of cultural information in the Middle Woodland Period, seeking to quantify how much cultural interaction could be detected within the archaeological record. Similarly, @Renfrew1983 explored the idea of culture as a communication system, where information flows between individuals and groups. He applied Shannon's concept of information transmission to study how cultural signals travel and degrade over time, though he acknowledged the complexity of non-linear dynamics in human societies, which challenge the assumptions of equilibrium-based models.

By the 21st century, the use of information theory in archaeology had broadened, particularly in studies of cultural transmission. @Crema2016 advanced Shannon's ideas by applying equilibrium and non-equilibrium models to study cultural transmission from frequency data. They used these models to reveal how cultural traits spread and stabilize within populations, providing quantitative insights into processes that are often difficult to observe directly in the archaeological record. Similarly, @Carrignon2023 used information theory to estimate transmission rates, applying Shannon's communication model to measure the uncertainty associated with the diffusion of cultural traits.

A major critique of these applications, as articulated by Raab and Goodyear (1984), concerns the oversimplification of human behaviors when abstract models like those derived from Shannon's theories are applied. They argue that middle-range theory, which often uses these models, fails to capture the full complexity of human action. @Zubrow1972 similarly critiqued the difficulty of accounting for environmental and social variables when applying information-theoretic frameworks. Despite this, modern scholars such as @Nolan2020 have worked to refine these models. Nolan assessed entropy, noise, and channel capacity to evaluate the significance of archaeological data, particularly focusing on how much information about past societies could be accurately recovered from the fragmented and noisy record.

More recently, @Gheorghiade2023 expanded Shannon's concept of entropy into a framework they called "Entropology," which applies entropy measures to better understand archaeological data. They critique the traditional applications of information theory for focusing too narrowly on entropy without accounting for the broader complexity and uncertainty of archaeological contexts. This critique echoes the central debate surrounding the use of information theory in archaeology: while it offers valuable tools for formalizing the study of cultural transmission and data integrity, it often struggles to capture the intricate and chaotic nature of human history.

In summary, the use of Shannon's information theory in archaeology has evolved from early models of data degradation and cultural transmission to more sophisticated frameworks that incorporate entropy and uncertainty. Scholars like Schiffer, Justeson, and Renfrew laid the foundation, while modern researchers like Nolan, Crema, and Gheorghiade have expanded these concepts to address the challenges posed by incomplete and noisy archaeological records. However, the ongoing debate highlights the tension between the precision offered by information theory and the complex realities of human history, questioning the extent to which these mathematical models can truly capture the richness of the past.

| #  | Paper                                                                                      | Key Insight                                                                                                                                              | Citations |
|----|---------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|
| 1  | [Crema, Kandler & Shennan (2016)](https://doi.org/10.1038/srep39122)                        | Applied equilibrium and non-equilibrium models to cultural transmission, revealing cultural patterns through frequency data.                               | -         |
| 2  | [Justeson (1973)](https://doi.org/10.2307/279360)                                           | Explored the limitations of information theory in archaeological inference, emphasizing the role of noise and uncertainty in reconstructing behavior.       | -         |
| 3  | [Nolan (2020)](https://doi.org/10.1007/s11135-020-00980-0)                                  | Assessed entropy, noise, and channel capacity in evaluating the significance of archaeological data and its degradation over time.                         | -         |
| 4  | [Schiffer (1972, 1983)](https://doi.org/10.2307/278203)                                     | Investigated how post-depositional processes disrupt information flow in artifacts, introducing the concept of formation processes in archaeological theory.| -         |
| 5  | [Carrignon, Bentley & O'Brien (2023)](https://doi.org/10.1016/j.jaa.2023.101545)            | Used information theory to estimate cultural transmission rates and measure the uncertainty in archaeological data.                                        | -         |
| 6  | [Raab & Goodyear (1984)](https://doi.org/10.2307/280018)                                    | Critiqued the over-reliance on middle-range theories and abstract models in capturing the complexity of human behaviors in archaeological records.          | -         |
| 7  | [Gheorghiade et al. (2023)](https://doi.org/10.1007/s10816-023-09627-4)                    | Proposed the concept of "Entropology" to move beyond simple entropy measures, emphasizing the need to account for complexity and uncertainty in the data.    | -         | -->
</section>
<section id="channels-classification-and-signal" class="level2">
<h2 class="anchored" data-anchor-id="channels-classification-and-signal">Channels, Classification, and Signal</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Justeson_1973_figure_1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Schematic representation of information transmission <span class="citation" data-cites="Justeson1973">(Fig. 1 <a href="#ref-Justeson1973" role="doc-biblioref">Justeson 1973, 133</a>)</span>.</figcaption>
</figure>
</div>
<p><span class="math display">\[
\biggl \lbrace (u_1, A_1), \ldots, (u_N, A_N) \biggr \rbrace
\]</span></p>
<p><span class="math display">\[
P \big \lbrace v(u_i) \in A_i \big \rbrace \geqslant 1 - \lambda, i=1, \ldots,N
\]</span></p>
<p><span class="math display">\[
C = \max_{\pi} \left\lbrace \sum_j \left\lbrack\sum_i \pi_i w(j|i) \log_2 \sum_i \pi_i w(j|i) - \sum_i \pi_i w(j|i) \log_2 \sum_i \pi_i w(j|i)\right\rbrack \right\rbrace
\]</span></p>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<section id="extrapolation-of-the-prehistoric-distribution-of-design-elements" class="level3">
<h3 class="anchored" data-anchor-id="extrapolation-of-the-prehistoric-distribution-of-design-elements">Extrapolation of the Prehistoric Distribution of Design Elements</h3>
<p><span class="math display">\[
M_t = \sum^{T}_{i=t} N_i
\]</span></p>
<p><span class="math display">\[
L_t = \sum^{T}_{i=t} M_i
\]</span></p>
</section>
<section id="noise" class="level3">
<h3 class="anchored" data-anchor-id="noise">Noise</h3>
<!-- calculate noise factor for each design element -->
<p><span class="math display">\[
\begin{aligned}
    \psi(r) &amp;= P(\text{receiving design element r given that r was sent})\\
    &amp;=P(\text{receiving r} \ | \ \text{r was sent})
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
P(A|B) = &amp;P(A \ \text{and} \ B) \div P(B) \text{, so} \\
&amp;P(\text{r sent} \ | \ \text{r received}) \cdot P(\text{r received})\\
= &amp;P(\text{r sent and r received})\\
= &amp;P(\text{r received} \ | \ \text{r sent}) \cdot P(\text{r sent}) \\
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\min_r \psi(r) &amp;= \min_r P(\text{r received} \ | \ \text{r sent}) \\
&amp;= \min_r 1 - P(\text{r not received} \ | \ \text{r sent}) \\
&amp;= \max_r P(\text{r not received} \ | \ \text{r sent}) \\
&amp;= 1- \lambda
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\overline{\psi} &amp;= \sum_r p_E(r) \psi(r) \\
&amp;= \sum_r p_E(r) \left \lbrack p(r) \div p_E(r) \right \rbrack \\
&amp;= \sum_r p(r) = 1
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
H' &amp;= -\sum_{i=1}^k p(x_i) \log_2 p(x_i) \\
&amp;= -\sum_{i=1}^k \frac{1}{k} \log_2 \frac{1}{k} \\
&amp;= -\log_2 \frac{1}{k} \\
&amp;= \log_2 k
\end{aligned}
\]</span></p>
<p><span class="math display">\[
h = H/H' \ \text{and} \ h_E = H_E/H'_E
\]</span></p>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References Cited</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Justeson1973" class="csl-entry" role="listitem">
Justeson, John S. 1973. <span>“Limitations of Archaeological Inference: An Information-Theoretic Approach with Applications in Methodology.”</span> <em>American Antiquity</em> 38 (2): 131–49. <a href="https://doi.org/10.2307/279360">https://doi.org/10.2307/279360</a>.
</div>
<div id="ref-Schiffer1972" class="csl-entry" role="listitem">
Schiffer, Michael B. 1972. <span>“Archaeological Context and Systemic Context.”</span> <em>American Antiquity</em> 37 (2): 156–65. <a href="https://doi.org/10.2307/278203">https://doi.org/10.2307/278203</a>.
</div>
<div id="ref-Schiffer1983" class="csl-entry" role="listitem">
———. 1983. <span>“Toward the Identification of Formation Processes.”</span> <em>American Antiquity</em> 48 (4): 675–706. <a href="https://doi.org/10.2307/279771">https://doi.org/10.2307/279771</a>.
</div>
<div id="ref-Schiffer1987" class="csl-entry" role="listitem">
———. 1987. <em>Formation Processes of the Archaeological Record</em>. University of New Mexico Press.
</div>
<div id="ref-Shannon1948" class="csl-entry" role="listitem">
Shannon, Claude E. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>Bell System Technical Journal</em> 27 (4): 623–56. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb00917.x">https://doi.org/10.1002/j.1538-7305.1948.tb00917.x</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="math inline">\(\log_2\)</span> refers to the base-2 logarithm.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>